"""Semantic affordance map (SAM) helpers for the town simulation.

The latest requirement is that the in-game town layout is computed directly
from reference artwork using Meta's `SAM 2 <https://ai.meta.com/sam2/>`_
segmentation models.  This module therefore contains a small geometry pipeline
that can:

* Run SAM 2 over a map illustration and classify the discovered segments into
  simulation-friendly zone categories.
* Project the segmentation into the tile grid used by :class:`smallville_gui`
  so that agent path-finding operates entirely on SAM-derived walkable areas.
* Derive navigation nodes near important junctions and venues so the GUI can
  continue to surface semantic points of interest.

To keep bootstrapping friendly, the module still understands the legacy JSON
format (generated by older experiments) so that existing caches remain usable.
When a SAM 2 environment is available, :meth:`SemanticAffordanceMap.from_image`
performs the full image-to-layout conversion and optionally stores the result
as JSON for subsequent quick loads.
"""

from __future__ import annotations

import json
from dataclasses import dataclass
import re
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import numpy as np
from PIL import Image

try:  # Optional heavy dependencies – handled gracefully when missing.
    import torch
except Exception:  # pragma: no cover - torch may be unavailable on tiny envs
    torch = None  # type: ignore[assignment]

try:
    from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator
except Exception:  # pragma: no cover - SAM 2 optional install
    SAM2AutomaticMaskGenerator = None  # type: ignore[assignment]

try:  # Hugging Face helpers for offline-friendly checkpoints
    from huggingface_hub import hf_hub_download, list_repo_files
except Exception:  # pragma: no cover - optional dependency handled gracefully
    hf_hub_download = None  # type: ignore[assignment]
    list_repo_files = None  # type: ignore[assignment]

Coordinate = Tuple[float, float]
GridRow = str


@dataclass
class Zone:
    """Semantic description of a region on the map."""

    id: str
    kind: str
    name: str
    polygon: Sequence[Coordinate]
    summary: str
    preferred_tiles: Sequence[str]


@dataclass
class PathNode:
    """Node in the navigation graph derived from the SAM annotations."""

    id: str
    position: Tuple[int, int]
    connections: Sequence[str]


DEFAULT_MODEL_ID = "facebook/sam2.1-hiera-small"
DEFAULT_TARGET_COLS = 32
DEFAULT_TARGET_ROWS = 22
SAM_MODEL_ROOT = Path(__file__).resolve().parents[1] / "Assets" / "models" / "sam2"


class SemanticAffordanceMap:
    """Loads SAM-derived semantics and exposes convenient helpers."""

    def __init__(
        self,
        *,
        zones: List[Zone],
        grid: Sequence[GridRow],
        nodes: List[PathNode],
        metadata: Optional[Dict[str, object]] = None,
    ) -> None:
        self.zones = zones
        self.grid = list(grid)
        self.nodes = nodes
        self.metadata = metadata or {}

    # ------------------------------------------------------------------
    # Construction helpers
    # ------------------------------------------------------------------

    def to_dict(self) -> Dict[str, object]:
        """Serialise the SAM map for caching."""

        return {
            "zones": [
                {
                    "id": zone.id,
                    "kind": zone.kind,
                    "name": zone.name,
                    "polygon": [list(point) for point in zone.polygon],
                    "summary": zone.summary,
                    "preferred_tiles": list(zone.preferred_tiles),
                }
                for zone in self.zones
            ],
            "grid": {"rows": list(self.grid)},
            "navmesh": [
                {
                    "id": node.id,
                    "position": list(node.position),
                    "connections": list(node.connections),
                }
                for node in self.nodes
            ],
            "meta": self.metadata,
        }

    @classmethod
    def from_file(cls, path: Path) -> "SemanticAffordanceMap":
        data = json.loads(path.read_text())
        zones = [
            Zone(
                id=entry["id"],
                kind=entry["kind"],
                name=entry["name"],
                polygon=[tuple(point) for point in entry["polygon"]],
                summary=entry.get("summary", ""),
                preferred_tiles=entry.get("preferred_tiles", ()),
            )
            for entry in data.get("zones", [])
        ]
        grid = data.get("grid", {}).get("rows", [])
        nodes = [
            PathNode(
                id=entry["id"],
                position=tuple(entry["position"]),
                connections=entry.get("connections", []),
            )
            for entry in data.get("navmesh", [])
        ]
        metadata = data.get("meta", {})
        return cls(zones=zones, grid=grid, nodes=nodes, metadata=metadata)

    @classmethod
    def from_image(
        cls,
        image_path: Path,
        *,
        model_id: str = DEFAULT_MODEL_ID,
        cache_path: Optional[Path] = None,
        target_cols: int = DEFAULT_TARGET_COLS,
        target_rows: int = DEFAULT_TARGET_ROWS,
    ) -> "SemanticAffordanceMap":
        """Create a semantic map by running SAM 2 over ``image_path``.

        Parameters
        ----------
        image_path:
            Illustration of the town.  Contrasting colours work best so SAM 2 can
            separate walkways from zones.
        model_id:
            Hugging Face model identifier to use with
            :class:`sam2.SAM2AutomaticMaskGenerator`.
        cache_path:
            Optional JSON file where the generated map is stored.  If the cache
            exists it is returned immediately.
        target_cols / target_rows:
            Desired tile resolution.  Should match the expectations from
            :class:`smallville_gui.TownMap` so that sprites align with the
            background art.
        """

        if cache_path and cache_path.exists():
            return cls.from_file(cache_path)

        if SAM2AutomaticMaskGenerator is None:
            raise RuntimeError(
                "SAM 2 is not available. Install the `sam2` package to enable "
                "image-driven layout generation."
            )

        if torch is None:
            raise RuntimeError(
                "PyTorch is required to run SAM 2. Install `torch` before "
                "invoking SemanticAffordanceMap.from_image()."
            )

        image = Image.open(image_path).convert("RGB")
        np_image = np.array(image)
        device = "cuda" if torch.cuda.is_available() else "cpu"

        checkpoint_dir = _ensure_sam_checkpoint(model_id)
        mask_kwargs = dict(
            device=device,
            apply_postprocessing=True,
            min_mask_region_area=256,
            points_per_side=24,
        )
        if checkpoint_dir:
            mask_kwargs["checkpoint_dir"] = str(checkpoint_dir)

        mask_generator = SAM2AutomaticMaskGenerator.from_pretrained(
            model_id,
            **mask_kwargs,
        )
        with torch.no_grad():  # type: ignore[union-attr]
            mask_records = mask_generator.generate(np_image)

        classifier = _SegmentClassifier()
        segments = classifier.process_masks(mask_records, np_image)

        grid_rows, zones = _project_segments_to_grid(
            segments,
            np_image.shape[1],
            np_image.shape[0],
            target_cols,
            target_rows,
        )
        nav_nodes = _build_nav_nodes(grid_rows, zones, classifier.walkable_codes)

        metadata: Dict[str, object] = {
            "source_image": str(image_path),
            "model_id": model_id,
            "device": device,
            "generated_segments": len(segments),
        }
        if checkpoint_dir:
            metadata["checkpoint_dir"] = str(checkpoint_dir)

        sam = cls(zones=zones, grid=grid_rows, nodes=nav_nodes, metadata=metadata)
        if cache_path:
            cache_path.write_text(json.dumps(sam.to_dict(), indent=2))
        return sam

    # ------------------------------------------------------------------
    # Convenience accessors
    # ------------------------------------------------------------------

    @property
    def width(self) -> int:
        return len(self.grid[0]) if self.grid else 0

    @property
    def height(self) -> int:
        return len(self.grid)

    def iter_zone_tiles(self) -> Iterable[Tuple[Zone, Tuple[int, int]]]:
        for zone in self.zones:
            for x, y in polygon_to_cells(zone.polygon):
                if 0 <= y < self.height and 0 <= x < self.width:
                    yield zone, (int(x), int(y))

    def to_layout(self, fallback: Sequence[str]) -> Sequence[str]:
        """Return the tile layout rows, falling back to a built-in layout."""

        if self.grid:
            return self.grid
        return fallback

    def zone_lookup(self) -> Dict[Tuple[int, int], Zone]:
        mapping: Dict[Tuple[int, int], Zone] = {}
        for zone, (x, y) in self.iter_zone_tiles():
            mapping[(x, y)] = zone
        return mapping

    def node_lookup(self) -> Dict[str, PathNode]:
        return {node.id: node for node in self.nodes}


# ---------------------------------------------------------------------------
# Geometry helpers
# ---------------------------------------------------------------------------


def polygon_to_cells(polygon: Sequence[Coordinate]) -> Iterable[Tuple[int, int]]:
    """Rasterise a convex polygon into integer cell coordinates."""

    if not polygon:
        return []
    xs = [p[0] for p in polygon]
    ys = [p[1] for p in polygon]
    min_x, max_x = int(min(xs)), int(max(xs))
    min_y, max_y = int(min(ys)), int(max(ys))

    for y in range(min_y, max_y + 1):
        for x in range(min_x, max_x + 1):
            if point_in_polygon((x + 0.5, y + 0.5), polygon):
                yield x, y


def point_in_polygon(point: Coordinate, polygon: Sequence[Coordinate]) -> bool:
    """Return ``True`` if the point lies inside the polygon using ray casting."""

    x, y = point
    inside = False
    n = len(polygon)
    px1, py1 = polygon[0]
    for i in range(n + 1):
        px2, py2 = polygon[i % n]
        if min(py1, py2) < y <= max(py1, py2) and x <= max(px1, px2):
            if py1 != py2:
                xinters = (y - py1) * (px2 - px1) / (py2 - py1) + px1
            else:
                xinters = px1
            if px1 == px2 or x <= xinters:
                inside = not inside
        px1, py1 = px2, py2
    return inside


__all__ = ["SemanticAffordanceMap", "Zone", "PathNode", "polygon_to_cells"]


def _polygon_centroid(polygon: Sequence[Coordinate]) -> Tuple[float, float]:
    """Return the centroid of a polygon."""

    if not polygon:
        return (0.0, 0.0)
    xs = [point[0] for point in polygon]
    ys = [point[1] for point in polygon]
    return (sum(xs) / len(xs), sum(ys) / len(ys))


def _normalise_model_id(model_id: str) -> str:
    return re.sub(r"[^a-zA-Z0-9_.-]+", "_", model_id)


def _ensure_sam_checkpoint(model_id: str) -> Optional[Path]:
    """Download SAM2 checkpoints into the project assets folder if possible."""

    checkpoint_dir = SAM_MODEL_ROOT / _normalise_model_id(model_id)
    checkpoint_dir.mkdir(parents=True, exist_ok=True)

    if hf_hub_download is None or list_repo_files is None:
        return checkpoint_dir

    try:
        files = list_repo_files(model_id)
    except Exception:  # pragma: no cover - remote listing may fail offline
        return checkpoint_dir

    wanted_suffixes = (".pt", ".pth", ".bin", ".yaml")
    for filename in files:
        if not filename.lower().endswith(wanted_suffixes):
            continue
        try:
            hf_hub_download(
                repo_id=model_id,
                filename=filename,
                local_dir=str(checkpoint_dir),
                local_dir_use_symlinks=False,
                resume_download=True,
            )
        except Exception:
            continue

    return checkpoint_dir


# ---------------------------------------------------------------------------
# SAM 2 helpers
# ---------------------------------------------------------------------------


@dataclass
class _SegmentSpec:
    tile_code: str
    kind: str
    base_name: str
    summary: str
    preferred_tiles: Sequence[str]
    color: Tuple[int, int, int]


@dataclass
class _Segment:
    mask: np.ndarray
    area: int
    mean_color: Tuple[float, float, float]
    spec: _SegmentSpec


class _SegmentClassifier:
    """Assign SAM segments to simulation-friendly categories."""

    segment_library: Tuple[_SegmentSpec, ...] = (
        _SegmentSpec(
            tile_code="T",
            kind="transport",
            base_name="Transit Link",
            summary="Primary walkable and drivable surfaces",
            preferred_tiles=("transport", "road"),
            color=(110, 110, 118),
        ),
        _SegmentSpec(
            tile_code="R",
            kind="road",
            base_name="Avenue",
            summary="Main streets threading through the district",
            preferred_tiles=("road", "transport"),
            color=(140, 140, 148),
        ),
        _SegmentSpec(
            tile_code="H",
            kind="residential",
            base_name="Residence",
            summary="Homes and shared housing terraces",
            preferred_tiles=("residential",),
            color=(164, 196, 252),
        ),
        _SegmentSpec(
            tile_code="C",
            kind="civic",
            base_name="Civic Plaza",
            summary="Town hall, library, and civic meeting points",
            preferred_tiles=("civic",),
            color=(250, 228, 176),
        ),
        _SegmentSpec(
            tile_code="P",
            kind="park",
            base_name="Parkland",
            summary="Green spaces and recreation lawns",
            preferred_tiles=("park", "leisure"),
            color=(124, 188, 108),
        ),
        _SegmentSpec(
            tile_code="O",
            kind="commerce",
            base_name="Market Row",
            summary="Shops, cafés, and community markets",
            preferred_tiles=("commerce",),
            color=(247, 177, 84),
        ),
        _SegmentSpec(
            tile_code="L",
            kind="leisure",
            base_name="Leisure Hub",
            summary="Culture, fitness, and entertainment venues",
            preferred_tiles=("leisure", "park"),
            color=(232, 197, 255),
        ),
        _SegmentSpec(
            tile_code="M",
            kind="health",
            base_name="Clinic",
            summary="Healthcare and wellbeing facilities",
            preferred_tiles=("health",),
            color=(255, 204, 204),
        ),
        _SegmentSpec(
            tile_code="I",
            kind="industry",
            base_name="Workshop",
            summary="Studios and light industry",
            preferred_tiles=("industry",),
            color=(182, 153, 255),
        ),
        _SegmentSpec(
            tile_code="A",
            kind="ai_lab",
            base_name="AI Lab",
            summary="Research campus for local innovators",
            preferred_tiles=("ai_lab", "civic"),
            color=(216, 168, 255),
        ),
    )

    def __init__(self) -> None:
        self._palette = np.array([spec.color for spec in self.segment_library], dtype=float)
        self.walkable_codes = {spec.tile_code for spec in self.segment_library if spec.kind in {"transport", "road", "park", "commerce", "leisure", "health", "ai_lab", "residential", "civic"}}

    def process_masks(
        self,
        mask_records: Sequence[Dict[str, object]],
        image: np.ndarray,
    ) -> List[_Segment]:
        segments: List[_Segment] = []
        # Sort by area descending so large, structural segments paint the grid first.
        sorted_records = sorted(
            mask_records,
            key=lambda entry: int(entry.get("area", 0)),
            reverse=True,
        )
        for record in sorted_records:
            mask = np.array(record["segmentation"], dtype=bool)
            area = int(record.get("area", int(mask.sum())))
            if area <= 0:
                continue
            mean_color = image[mask].mean(axis=0) if mask.any() else (0.0, 0.0, 0.0)
            spec = self._classify(mean_color)
            if spec is None:
                continue
            segments.append(
                _Segment(
                    mask=mask,
                    area=area,
                    mean_color=tuple(float(c) for c in mean_color),
                    spec=spec,
                )
            )
        return segments

    def _classify(self, color: Sequence[float]) -> Optional[_SegmentSpec]:
        if not color:
            return None
        sample = np.array(color)
        distances = np.linalg.norm(self._palette - sample, axis=1)
        index = int(np.argmin(distances))
        if float(distances[index]) > 80.0:
            # Ignore tiny artefacts that are far from our curated palette.
            return None
        return self.segment_library[index]


def _project_segments_to_grid(
    segments: Sequence[_Segment],
    image_width: int,
    image_height: int,
    target_cols: int,
    target_rows: int,
) -> Tuple[List[str], List[Zone]]:
    if target_cols <= 0 or target_rows <= 0:
        raise ValueError("target_cols and target_rows must be positive integers")

    raster = np.full((image_height, image_width), "T", dtype="U2")
    zone_polygons: Dict[str, List[Coordinate]] = {}
    zone_counts: Dict[str, int] = {}

    for segment in segments:
        raster[segment.mask] = segment.spec.tile_code
        if segment.spec.kind in {"transport", "road"}:
            # Transport layers encode walkability but are not exposed as POI zones.
            continue
        zone_counts.setdefault(segment.spec.kind, 0)
        zone_counts[segment.spec.kind] += 1
        zone_id = f"{segment.spec.kind}_{zone_counts[segment.spec.kind]}"
        zone_polygons[zone_id] = _mask_to_polygon(
            segment.mask,
            segment.spec,
            image_width,
            image_height,
            target_cols,
            target_rows,
        )

    cell_width = image_width / target_cols
    cell_height = image_height / target_rows
    grid_rows: List[str] = []

    for row in range(target_rows):
        y0 = int(round(row * cell_height))
        y1 = int(round((row + 1) * cell_height))
        y1 = min(y1, image_height)
        chars: List[str] = []
        for col in range(target_cols):
            x0 = int(round(col * cell_width))
            x1 = int(round((col + 1) * cell_width))
            x1 = min(x1, image_width)
            tile_slice = raster[y0:y1, x0:x1]
            if tile_slice.size == 0:
                chars.append("T")
                continue
            values, counts = np.unique(tile_slice, return_counts=True)
            tile_code = str(values[np.argmax(counts)])
            chars.append(tile_code or "T")
        grid_rows.append("".join(chars))

    zones: List[Zone] = []
    for zone_id, polygon in zone_polygons.items():
        spec_kind = zone_id.split("_", 1)[0]
        spec = next(spec for spec in _SegmentClassifier.segment_library if spec.kind == spec_kind)
        centroid = _polygon_centroid(polygon)
        zones.append(
            Zone(
                id=zone_id,
                kind=spec.kind,
                name=f"{spec.base_name} {zone_id.split('_')[-1]}",
                polygon=polygon,
                summary=spec.summary,
                preferred_tiles=tuple(spec.preferred_tiles),
            )
        )

    return grid_rows, zones


def _mask_to_polygon(
    mask: np.ndarray,
    spec: _SegmentSpec,
    image_width: int,
    image_height: int,
    target_cols: int,
    target_rows: int,
) -> List[Coordinate]:
    ys, xs = np.nonzero(mask)
    if ys.size == 0 or xs.size == 0:
        return []
    min_x, max_x = xs.min(), xs.max()
    min_y, max_y = ys.min(), ys.max()
    cell_width = image_width / target_cols
    cell_height = image_height / target_rows
    left = float(min_x) / cell_width
    right = float(max_x + 1) / cell_width
    top = float(min_y) / cell_height
    bottom = float(max_y + 1) / cell_height
    return [(left, top), (right, top), (right, bottom), (left, bottom)]


def _build_nav_nodes(
    grid_rows: Sequence[str],
    zones: Sequence[Zone],
    walkable_codes: Sequence[str],
) -> List[PathNode]:
    height = len(grid_rows)
    width = len(grid_rows[0]) if height else 0
    walkable_set = set(walkable_codes)
    walkable_cells = {
        (x, y)
        for y, row in enumerate(grid_rows)
        for x, code in enumerate(row)
        if code in walkable_set
    }

    node_positions: Dict[Tuple[int, int], str] = {}
    nodes: List[PathNode] = []

    def neighbor_coords(x: int, y: int) -> Iterable[Tuple[int, int]]:
        for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):
            nx, ny = x + dx, y + dy
            if 0 <= nx < width and 0 <= ny < height and (nx, ny) in walkable_cells:
                yield nx, ny

    stride = max(1, min(width, height) // 8)
    counter = 0
    for (x, y) in sorted(walkable_cells, key=lambda pt: (pt[1], pt[0])):
        degree = sum(1 for _ in neighbor_coords(x, y))
        if degree != 2 or (x % stride == 0 and y % stride == 0):
            counter += 1
            node_id = f"node_{counter}"
            node_positions[(x, y)] = node_id
            nodes.append(PathNode(id=node_id, position=(x, y), connections=[]))

    # Ensure there is always at least one waypoint.
    if not nodes and walkable_cells:
        (x, y) = next(iter(walkable_cells))
        node_positions[(x, y)] = "node_1"
        nodes.append(PathNode(id="node_1", position=(x, y), connections=[]))

    # Connect nodes along walkable corridors.
    def trace(start: Tuple[int, int], direction: Tuple[int, int]) -> Optional[str]:
        x, y = start
        dx, dy = direction
        while True:
            x += dx
            y += dy
            if (x, y) not in walkable_cells:
                return None
            node_id = node_positions.get((x, y))
            if node_id:
                return node_id

    for node in nodes:
        x, y = node.position
        connections = set(node.connections)
        for direction in ((1, 0), (-1, 0), (0, 1), (0, -1)):
            target = trace((x, y), direction)
            if target and target != node.id:
                connections.add(target)
        node.connections = sorted(connections)

    # Rename the top-most node to ``north_gate`` so existing GUI hooks keep working.
    if nodes:
        topmost = min(nodes, key=lambda node: (node.position[1], node.position[0]))
        old_id = topmost.id
        topmost.id = "north_gate"
        node_positions[tuple(topmost.position)] = topmost.id
        for node in nodes:
            node.connections = ["north_gate" if conn == old_id else conn for conn in node.connections]

    # Attach zone centroids to the nearest walkable waypoint.
    for zone in zones:
        centroid = _polygon_centroid(zone.polygon)
        nearest = _nearest_walkable(walkable_cells, centroid)
        if nearest is None:
            continue
        zone_node_id = zone.id
        nodes.append(
            PathNode(
                id=zone_node_id,
                position=(int(round(centroid[0])), int(round(centroid[1]))),
                connections=[node_positions.get(nearest, "north_gate")],
            )
        )

    return nodes


def _nearest_walkable(
    walkable_cells: Iterable[Tuple[int, int]],
    target: Tuple[float, float],
) -> Optional[Tuple[int, int]]:
    tx, ty = target
    best: Optional[Tuple[int, int]] = None
    best_dist = float("inf")
    for x, y in walkable_cells:
        dist = (x - tx) ** 2 + (y - ty) ** 2
        if dist < best_dist:
            best_dist = dist
            best = (x, y)
    return best
